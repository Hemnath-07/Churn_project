# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19mddwegh-XQc5fTkJ5vZKEnF-7iOdK7L
"""

import streamlit as st
import pandas as pd
import joblib
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier # For dummy model creation
import os # Import os module to handle directory creation

# --- Configuration and Setup ---
st.set_page_config(
    page_title="Customer Churn Predictor",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- IMPORTANT: Configure your model artifacts directory here ---
# This directory should contain 'churn_prediction_model.joblib', 'scaler.joblib',
# 'label_encoders.joblib', and 'feature_names.joblib'
# Example: If your model files are in a folder named 'models' in the same directory as app.py
# You should place your saved model files (from churn_model.py) into this directory.
MODEL_ARTIFACTS_DIR = 'model_artifacts' # You can change this to 'models', 'my_models', etc.

# Custom CSS for a more attractive look
st.markdown("""
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap');

    html, body, [class*="st-"] {
        font-family: 'Inter', sans-serif;
        color: #333;
    }
    .main {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
    }
    .stApp {
        background-color: #f0f2f6;
    }
    .stButton>button {
        background-color: #4CAF50; /* Green */
        color: white;
        padding: 10px 20px;
        border-radius: 8px;
        border: none;
        box-shadow: 2px 2px 5px rgba(0,0,0,0.2);
        transition: all 0.3s ease;
    }
    .stButton>button:hover {
        background-color: #45a049;
        box_shadow: 3px 3px 8px rgba(0,0,0,0.3);
        transform: translateY(-2px);
    }
    .stFileUploader {
        background-color: #e6eaf0;
        border-radius: 8px;
        padding: 15px;
        box-shadow: inset 0 0 5px rgba(0,0,0,0.1);
    }
    .stTextInput>div>div>input {
        border-radius: 8px;
        border: 1px solid #ccc;
        padding: 8px 12px;
    }
    .stSelectbox>div>div {
        border-radius: 8px;
        border: 1px solid #ccc;
    }
    .stExpander {
        background-color: #ffffff;
        border-radius: 10px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        padding: 15px;
        margin-bottom: 20px;
    }
    .stExpander > div > div > div > p {
        font-weight: 600;
        color: #2c3e50;
    }
    h1, h2, h3, h4, h5, h6 {
        color: #2c3e50;
    }
    .stAlert {
        border-radius: 8px;
    }
    </style>
    """, unsafe_allow_html=True)

# --- Function to Generate Dummy Model Artifacts (for initial run if no model is trained) ---
def generate_dummy_artifacts(model_dir):
    """
    Generates dummy model, scaler, label encoders, and feature names for initial app run.
    This allows the app to load and display even without a pre-trained model.
    """
    print(f"Generating dummy model artifacts in {model_dir}...")
    # Create the directory if it doesn't exist
    os.makedirs(model_dir, exist_ok=True)

    # Dummy Model
    dummy_model = RandomForestClassifier(n_estimators=10, random_state=42)
    joblib.dump(dummy_model, os.path.join(model_dir, 'churn_prediction_model.joblib'))

    # Dummy Scaler
    dummy_scaler = StandardScaler()
    # Fit on some dummy data to make it functional
    dummy_scaler.fit(np.array([[10, 50], [20, 100], [30, 150]]))
    joblib.dump(dummy_scaler, os.path.join(model_dir, 'scaler.joblib'))

    # Dummy Label Encoders
    dummy_le = LabelEncoder()
    dummy_le.fit(['Yes', 'No'])
    dummy_label_encoders = {
        'Gender': dummy_le,
        'Partner': dummy_le,
        'Dependents': dummy_le,
        'PhoneService': dummy_le,
        'MultipleLines': dummy_le,
        'OnlineSecurity': dummy_le,
        'OnlineBackup': dummy_le,
        'DeviceProtection': dummy_le,
        'TechSupport': dummy_le,
        'StreamingTV': dummy_le,
        'StreamingMovies': dummy_le,
        'PaperlessBilling': dummy_le,
        # Example for OHE features - these must match the expected features from your actual model
        'InternetService_Fiber optic': dummy_le,
        'InternetService_No': dummy_le,
        'Contract_One year': dummy_le,
        'Contract_Two year': dummy_le,
        'PaymentMethod_Credit card (automatic)': dummy_le,
        'PaymentMethod_Electronic check': dummy_le,
        'PaymentMethod_Mailed check': dummy_le,
        'Churn': dummy_le # Important for decoding predictions
    }
    joblib.dump(dummy_label_encoders, os.path.join(model_dir, 'label_encoders.joblib'))

    # Dummy Feature Names (must match expected features after preprocessing)
    dummy_feature_names = [
        'SeniorCitizen', 'Tenure', 'MonthlyCharges', 'TotalCharges',
        'Gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',
        'InternetService_Fiber optic', 'InternetService_No',
        'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',
        'StreamingTV', 'StreamingMovies', 'Contract_One year', 'Contract_Two year',
        'PaperlessBilling', 'PaymentMethod_Credit card (automatic)',
        'PaymentMethod_Electronic check', 'PaymentMethod_Mailed check'
    ]
    joblib.dump(dummy_feature_names, os.path.join(model_dir, 'feature_names.joblib'))
    print(f"Dummy artifacts created in {model_dir}.")


# --- Load Model Artifacts ---
@st.cache_resource # Cache the model loading for efficiency
def load_model_artifacts(model_dir):
    """Loads the pre-trained model, scaler, label encoders, and feature names."""
    model_path = os.path.join(model_dir, 'churn_prediction_model.joblib')
    scaler_path = os.path.join(model_dir, 'scaler.joblib')
    label_encoders_path = os.path.join(model_dir, 'label_encoders.joblib')
    feature_names_path = os.path.join(model_dir, 'feature_names.joblib')

    try:
        model = joblib.load(model_path)
        scaler = joblib.load(scaler_path)
        label_encoders = joblib.load(label_encoders_path)
        feature_names = joblib.load(feature_names_path)
        st.success(f"Model artifacts loaded successfully from '{model_dir}'!")
        return model, scaler, label_encoders, feature_names
    except FileNotFoundError:
        st.warning(f"Model artifacts not found in '{model_dir}'. Generating dummy files. Please run `churn_model.py` to train a real model and save it to this directory for accurate predictions.")
        generate_dummy_artifacts(model_dir)
        # Try loading again after generating dummies
        return load_model_artifacts(model_dir) # Recursive call to load the newly created dummy artifacts
    except Exception as e:
        st.error(f"Error loading model artifacts from '{model_dir}': {e}")
        st.info("Please ensure `churn_model.py` was run successfully to create the necessary files in the specified directory.")
        return None, None, None, None

# Load the model artifacts using the specified directory
model, scaler, label_encoders, feature_names = load_model_artifacts(MODEL_ARTIFACTS_DIR)

# --- Data Preprocessing Function (for new input data) ---
def preprocess_new_data(input_df, scaler, label_encoders, feature_names):
    """
    Preprocesses new input data using the loaded scaler and label encoders.
    This function must mirror the preprocessing steps in churn_model.py.
    """
    processed_df = input_df.copy()

    # Convert 'TotalCharges' to numeric, handle errors by coercing to NaN, then fill NaN with 0
    if 'TotalCharges' in processed_df.columns:
        processed_df['TotalCharges'] = pd.to_numeric(processed_df['TotalCharges'], errors='coerce')
        processed_df['TotalCharges'].fillna(0, inplace=True)

    # Apply Label Encoding for specific binary columns that were label encoded during training
    # We need to know which columns were label encoded.
    # For simplicity, we assume the label_encoders dictionary contains all relevant ones.
    binary_cols = [col for col, le in label_encoders.items() if col != 'Churn' and isinstance(le, LabelEncoder) and len(le.classes_) <= 2]
    for col in binary_cols:
        if col in processed_df.columns:
            # Handle unseen labels: if a category is not in le.classes_, it will raise an error.
            # A robust way is to map known values and leave others as NaN or a default, then impute.
            # For this demo, we assume valid inputs for simplicity.
            processed_df[col] = processed_df[col].apply(lambda x: label_encoders[col].transform([x])[0] if x in label_encoders[col].classes_ else -1)


    # Apply One-Hot Encoding for multi-category features
    # This requires careful handling to ensure the same columns are generated as during training.
    # We'll identify original categorical features from the feature_names list.
    # A better approach would be to save the OneHotEncoder object itself.
    # For now, we'll manually identify and create dummies.

    # Identify original categorical columns that were one-hot encoded
    # This is a heuristic based on feature_names. A more robust solution involves saving the OHE object.
    original_cat_cols_for_ohe = []
    # Example: 'InternetService_Fiber optic' implies 'InternetService' was OHE
    for feature in feature_names:
        if '_' in feature and feature.split('_')[0] not in binary_cols and feature.split('_')[0] not in ['SeniorCitizen', 'Tenure', 'MonthlyCharges', 'TotalCharges']:
            original_cat_cols_for_ohe.append(feature.split('_')[0])
    original_cat_cols_for_ohe = list(set(original_cat_cols_for_ohe)) # Remove duplicates

    # Create dummy variables for the input data for these multi-category features
    processed_df = pd.get_dummies(processed_df, columns=original_cat_cols_for_ohe, drop_first=True)

    # Align columns with training data features (CRITICAL STEP!)
    # Add missing columns (from training set) to input_df and fill with 0
    # Drop extra columns (if any) from input_df that were not in training set
    missing_cols = set(feature_names) - set(processed_df.columns)
    for c in missing_cols:
        processed_df[c] = 0
    # Ensure the order of columns is exactly the same as during training
    processed_df = processed_df[feature_names]

    # Scale numerical features
    numerical_features_to_scale = [col for col in processed_df.columns if col in scaler.feature_names_in_]
    processed_df[numerical_features_to_scale] = scaler.transform(processed_df[numerical_features_to_scale])

    return processed_df


# --- App Title and Introduction ---
st.title("ðŸ“Š Customer Churn Prediction Dashboard")
st.markdown("""
    Welcome to the Customer Churn Prediction Dashboard!
    Upload your customer data CSV file to get insights and predict churn likelihood.
    You can also input individual customer details for a real-time prediction.
""")

# --- Sidebar for File Upload and Individual Prediction ---
with st.sidebar:
    st.header("Upload Data or Predict Individual")

    uploaded_file = st.file_uploader("Upload your Customer Data CSV", type=["csv"])

    st.markdown("---")
    st.subheader("Predict for a Single Customer")

    # Input fields for individual prediction
    st.markdown("Enter customer details below:")
    col1, col2 = st.columns(2)
    with col1:
        gender = st.selectbox("Gender", ['Male', 'Female'])
        senior_citizen = st.selectbox("Senior Citizen", [0, 1], format_func=lambda x: "Yes" if x==1 else "No")
        partner = st.selectbox("Partner", ['Yes', 'No'])
        dependents = st.selectbox("Dependents", ['Yes', 'No'])
        tenure = st.slider("Tenure (Months)", 0, 72, 12)
        phone_service = st.selectbox("Phone Service", ['Yes', 'No'])
        multiple_lines = st.selectbox("Multiple Lines", ['Yes', 'No', 'No phone service'])
        internet_service = st.selectbox("Internet Service", ['DSL', 'Fiber optic', 'No'])
        online_security = st.selectbox("Online Security", ['Yes', 'No', 'No internet service'])
        online_backup = st.selectbox("Online Backup", ['Yes', 'No', 'No internet service'])

    with col2:
        device_protection = st.selectbox("Device Protection", ['Yes', 'No', 'No internet service'])
        tech_support = st.selectbox("Tech Support", ['Yes', 'No', 'No internet service'])
        streaming_tv = st.selectbox("Streaming TV", ['Yes', 'No', 'No internet service'])
        streaming_movies = st.selectbox("Streaming Movies", ['Yes', 'No', 'No internet service'])
        contract = st.selectbox("Contract", ['Month-to-month', 'One year', 'Two year'])
        paperless_billing = st.selectbox("Paperless Billing", ['Yes', 'No'])
        payment_method = st.selectbox("Payment Method", ['Electronic check', 'Mailed check', 'Bank transfer (automatic)', 'Credit card (automatic)'])
        monthly_charges = st.number_input("Monthly Charges", min_value=0.0, max_value=200.0, value=70.0, step=0.1)
        total_charges = st.number_input("Total Charges", min_value=0.0, max_value=10000.0, value=800.0, step=0.1)

    predict_button = st.button("Predict Individual Churn")

# --- Main Content Area ---
if uploaded_file is not None:
    try:
        data = pd.read_csv(uploaded_file)
        st.success("CSV file uploaded successfully!")

        st.subheader("Dataset Overview")
        st.write(f"Shape of dataset: {data.shape[0]} rows, {data.shape[1]} columns")
        st.dataframe(data.head())

        st.subheader("Data Statistics")
        st.write(data.describe())

        # --- Visualizations ---
        st.header("Visual Churn Analytics")

        # Check if 'Churn' column exists for visualizations
        if 'Churn' in data.columns:
            # 1. Churn Distribution (Pie Chart)
            st.markdown("### 1. Churn Distribution")
            churn_counts = data['Churn'].value_counts()
            fig1, ax1 = plt.subplots(figsize=(6, 6))
            ax1.pie(churn_counts, labels=churn_counts.index, autopct='%1.1f%%', startangle=90,
                    colors=['#4CAF50', '#FF6347'], explode=[0.05, 0]) # Green for No Churn, Red for Churn
            ax1.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.
            ax1.set_title('Distribution of Customer Churn', fontsize=16)
            st.pyplot(fig1)

            # 2. Churn by Contract Type (Bar Chart)
            st.markdown("### 2. Churn by Contract Type")
            if 'Contract' in data.columns:
                contract_churn = data.groupby('Contract')['Churn'].value_counts(normalize=True).unstack()
                fig2, ax2 = plt.subplots(figsize=(10, 6))
                contract_churn.plot(kind='bar', stacked=True, ax=ax2, color={'No': '#4CAF50', 'Yes': '#FF6347'})
                ax2.set_title('Churn Rate by Contract Type', fontsize=16)
                ax2.set_ylabel('Proportion', fontsize=12)
                ax2.set_xlabel('Contract Type', fontsize=12)
                ax2.tick_params(axis='x', rotation=45)
                ax2.legend(title='Churn')
                plt.tight_layout()
                st.pyplot(fig2)
            else:
                st.info(" 'Contract' column not found for visualization.")

            # 3. Monthly Charges vs. Churn (Box Plot)
            st.markdown("### 3. Monthly Charges vs. Churn")
            if 'MonthlyCharges' in data.columns:
                fig3, ax3 = plt.subplots(figsize=(10, 6))
                sns.boxplot(x='Churn', y='MonthlyCharges', data=data, ax=ax3, palette={'No': '#4CAF50', 'Yes': '#FF6347'})
                ax3.set_title('Monthly Charges Distribution by Churn Status', fontsize=16)
                ax3.set_xlabel('Churn', fontsize=12)
                ax3.set_ylabel('Monthly Charges', fontsize=12)
                plt.tight_layout()
                st.pyplot(fig3)
            else:
                st.info(" 'MonthlyCharges' column not found for visualization.")

            # 4. Tenure vs. Churn (Violin Plot)
            st.markdown("### 4. Tenure vs. Churn")
            if 'Tenure' in data.columns:
                fig4, ax4 = plt.subplots(figsize=(10, 6))
                sns.violinplot(x='Churn', y='Tenure', data=data, ax=ax4, palette={'No': '#4CAF50', 'Yes': '#FF6347'})
                ax4.set_title('Tenure Distribution by Churn Status', fontsize=16)
                ax4.set_xlabel('Churn', fontsize=12)
                ax4.set_ylabel('Tenure (Months)', fontsize=12)
                plt.tight_layout()
                st.pyplot(fig4)
            else:
                st.info(" 'Tenure' column not found for visualization.")


            # Additional Feature Distributions (e.g., Internet Service)
            st.markdown("### 5. Internet Service Distribution")
            if 'InternetService' in data.columns:
                fig5, ax5 = plt.subplots(figsize=(8, 5))
                sns.countplot(x='InternetService', hue='Churn', data=data, ax=ax5, palette={'No': '#4CAF50', 'Yes': '#FF6347'})
                ax5.set_title('Internet Service Distribution by Churn', fontsize=16)
                ax5.set_xlabel('Internet Service', fontsize=12)
                ax5.set_ylabel('Count', fontsize=12)
                plt.tight_layout()
                st.pyplot(fig5)
            else:
                st.info(" 'InternetService' column not found for visualization.")


        else:
            st.warning("The uploaded CSV does not contain a 'Churn' column. Cannot display churn-related analytics.")

    except Exception as e:
        st.error(f"Error processing uploaded file: {e}")
        st.info("Please ensure your CSV is correctly formatted.")

elif predict_button:
    st.subheader("Individual Customer Churn Prediction Result")
    if model is None or scaler is None or label_encoders is None or feature_names is None:
        st.error("Model artifacts are not loaded. Cannot make predictions. Please check the `MODEL_ARTIFACTS_DIR` configuration.")
    else:
        input_data = {
            'Gender': gender,
            'SeniorCitizen': senior_citizen,
            'Partner': partner,
            'Dependents': dependents,
            'Tenure': tenure,
            'PhoneService': phone_service,
            'MultipleLines': multiple_lines,
            'InternetService': internet_service,
            'OnlineSecurity': online_security,
            'OnlineBackup': online_backup,
            'DeviceProtection': device_protection,
            'TechSupport': tech_support,
            'StreamingTV': streaming_tv,
            'StreamingMovies': streaming_movies,
            'Contract': contract,
            'PaperlessBilling': paperless_billing,
            'PaymentMethod': payment_method,
            'MonthlyCharges': monthly_charges,
            'TotalCharges': total_charges
        }
        input_df = pd.DataFrame([input_data])

        try:
            processed_input = preprocess_new_data(input_df, scaler, label_encoders, feature_names)
            prediction = model.predict(processed_input)
            probability = model.predict_proba(processed_input)[:, 1] # Probability of churn (class 1)

            churn_status = label_encoders['Churn'].inverse_transform(prediction)[0]
            churn_probability = probability[0]

            if churn_status == 'Yes':
                st.error(f"**Prediction: This customer is likely to CHURN!** ðŸ“‰")
                st.markdown(f"**Churn Probability:** `{churn_probability:.2%}`")
                st.info("Consider proactive retention strategies for this customer.")
            else:
                st.success(f"**Prediction: This customer is likely to NOT CHURN.** ðŸŽ‰")
                st.markdown(f"**Churn Probability:** `{churn_probability:.2%}`")
                st.info("Great! Continue monitoring customer satisfaction.")

            st.markdown("---")
            st.markdown("#### Input Data Used for Prediction:")
            st.dataframe(input_df)

        except Exception as e:
            st.error(f"An error occurred during prediction: {e}")
            st.info("Please check the input values and ensure the model artifacts are valid and match the expected format.")

else:
    st.info("Upload a CSV file or fill in the customer details in the sidebar to get started!")